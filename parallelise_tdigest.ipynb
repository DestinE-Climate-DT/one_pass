{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l40055.lvt.dkrz.de\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import glob \n",
    "import os \n",
    "#import pyarrow as pa \n",
    "#from tdigest import TDigest\n",
    "from crick import TDigest as TDigest_cr \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import Process\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "os.chdir('/home/b/b382291/git/AQUA') # CHANGE TO CORRECT PATH OR REMOVE IF USING OTHER DATA \n",
    "#imported_module = importlib.import_module(\"aqua\")\n",
    "from aqua import Reader\n",
    "from aqua.reader import catalogue\n",
    "\n",
    "!hostname"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use ICON data \n",
    "\n",
    "In this example using ICON data over a 20 million grid cells. This can be any data however, the data isn't really important here. Another example below with data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date: 2020-01-20T00:00:00.000000000 stop_date: 2020-01-20T00:30:00.000000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reader = Reader(model=\"ICON\", exp=\"ngc2009\", source=\"atm_2d_ml_R02B09\")\n",
    "reader.reset_stream()\n",
    "data_gen = reader.retrieve(streaming_generator=True, stream_step=1, stream_unit = \"hours\")\n",
    "count = 0 \n",
    "\n",
    "for data in data_gen:\n",
    "    print(f\"start_date: {data.time[0].values} stop_date: {data.time[-1].values}\")\n",
    "    data = data.sp\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[100823.94 , 101063.91 , 100732.24 , ..., 100901.18 , 100901.18 ,\n",
       "        100901.18 ],\n",
       "       [100824.8  , 101064.89 , 100738.81 , ..., 100901.19 , 100909.35 ,\n",
       "        100901.805]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.compute() \n",
    "ds = data.values# [:, 0:10000000] # extract the numpy array \n",
    "print(type(ds))\n",
    "weight, array_length= np.shape(ds)\n",
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If AQUA above doesn't work just use a random array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = np.random.rand(10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Running sequential \n",
    "\n",
    "initalising digests takes around 17 s \n",
    "doing the udpate takes around 1 m 15 s for full array \n",
    "\n",
    "for 10 million grid cells, init around 10 s, update 35 s (whole thing 42s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 109719/10000000 [00:00<00:09, 1097141.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [00:08<00:00, 1174068.42it/s]\n",
      "100%|██████████| 10000000/10000000 [00:35<00:00, 280746.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 35.62069272994995 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "digest_list = []\n",
    "\n",
    "for j in tqdm.tqdm(range(array_length)):\n",
    "   # initalising digests \n",
    "   digest_list.append(TDigest_cr(compression=1))\n",
    "   \n",
    "start_time = time.time() \n",
    "for j in tqdm.tqdm(range(array_length)):\n",
    "   digest_list[j].update(ds[:,j])\n",
    "print('time', time.time() - start_time, 's')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below attemping to parellelise using Pipes. The initilisation isn't parellelised, that's still sequential for simplicity as it's quicker. When you uncomment the Pipes communicator this takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20971520/20971520 [00:17<00:00, 1182262.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 9.243178367614746 s\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pipe \n",
    "\n",
    "def update_digest(job_length, ds_short, short_list): #, connection):\n",
    "\n",
    "    [short_list[j].update(ds_short[:, j]) for j in range(job_length)]\n",
    "    #connection.send(short_list)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    digest_list = []\n",
    "    for j in tqdm.tqdm(range(array_length)):\n",
    "       # initalising digests \n",
    "       digest_list.append(TDigest_cr(compression=1))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Split the work among processes\n",
    "    num_processes = 22  # this makes it explict \n",
    "    chunk_size = array_length // num_processes\n",
    "    processes = []\n",
    "    #pipes = []\n",
    "        \n",
    "    for i in range(num_processes):\n",
    "        #conn1, conn2 = Pipe()\n",
    "        #pipes.append(conn1)\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i < num_processes - 1 else array_length\n",
    "        job_length = end - start\n",
    "        job_args = (job_length, ds[:, start:end], digest_list[start:end]) #, conn2)\n",
    "        process =  Process(target=update_digest, args=job_args)\n",
    "        processes.append(process)\n",
    "\n",
    "    for process in processes:\n",
    "        process.start()\n",
    "    \n",
    "    # data_chunks = []\n",
    "    # for conn1 in pipes:        \n",
    "    #     try:\n",
    "    #         data_chunk = conn1.recv()\n",
    "    #         data_chunks.extend(data_chunk)    # read the data from the pipe\n",
    "\n",
    "    #     except EOFError as err:\n",
    "    #         print('Got here')\n",
    "    #     except OSError as err: \n",
    "    #         print('OSError')\n",
    "            \n",
    "    for process in processes:\n",
    "        process.join()\n",
    "        \n",
    "    print('time', time.time() - start_time, 's')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried to use a shared memory object but I can't find a c type object that suports the t-digests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 1087489.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unicode string or integer address expected instead of crick.tdigest.TDigest instance",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m     data\u001b[39m.\u001b[39mappend(TDigest_cr(compression\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     34\u001b[0m \u001b[39m#array = RawArray(ctypes.py_object, digest_list)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# get ctype for our array\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# create ctype array initialized from our array\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m array \u001b[39m=\u001b[39m Array(ctypes\u001b[39m.\u001b[39;49mc_wchar_p, data, lock\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     38\u001b[0m \u001b[39m# confirm contents of the shared array\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(array[:\u001b[39m10\u001b[39m], \u001b[39mlen\u001b[39m(array))\n",
      "File \u001b[0;32m~/.conda/envs/aqua/lib/python3.10/multiprocessing/sharedctypes.py:88\u001b[0m, in \u001b[0;36mArray\u001b[0;34m(typecode_or_type, size_or_initializer, lock, ctx)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mArray\u001b[39m(typecode_or_type, size_or_initializer, \u001b[39m*\u001b[39m, lock\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ctx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m    Return a synchronization wrapper for a RawArray\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     obj \u001b[39m=\u001b[39m RawArray(typecode_or_type, size_or_initializer)\n\u001b[1;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m lock \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.conda/envs/aqua/lib/python3.10/multiprocessing/sharedctypes.py:67\u001b[0m, in \u001b[0;36mRawArray\u001b[0;34m(typecode_or_type, size_or_initializer)\u001b[0m\n\u001b[1;32m     65\u001b[0m type_ \u001b[39m=\u001b[39m type_ \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(size_or_initializer)\n\u001b[1;32m     66\u001b[0m result \u001b[39m=\u001b[39m _new_value(type_)\n\u001b[0;32m---> 67\u001b[0m result\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49msize_or_initializer)\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: unicode string or integer address expected instead of crick.tdigest.TDigest instance"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "from multiprocessing.sharedctypes import Array\n",
    "from numpy import ones\n",
    "import ctypes \n",
    "\n",
    "# task executed in a child process\n",
    "def task(array, ds):\n",
    "    # check some data in the array\n",
    "    print(array[:10], len(array))\n",
    "    # change data in the array\n",
    "\n",
    "    for i in range(len(array)):\n",
    "        array[i] = ds[1,j]\n",
    "        #array[i].update(ds[:, i])\n",
    "        \n",
    "    # confirm the data was changed\n",
    "    print(array[300:310], len(array))\n",
    "    print('changed')\n",
    " \n",
    "# protect the entry point\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # define the size of the numpy array\n",
    "    n = 10000\n",
    "    # create the numpy array\n",
    "    data = ones((n,))\n",
    "    #print(data[:10], data.shape)\n",
    "    \n",
    "    data = []\n",
    "    for j in tqdm.tqdm(range(array_length)):\n",
    "        # initalising digests \n",
    "        data.append(TDigest_cr(compression=1))\n",
    "        \n",
    "    #array = RawArray(ctypes.py_object, digest_list)\n",
    "    # get ctype for our array\n",
    "    # create ctype array initialized from our array\n",
    "    array = Array(ctypes.c_wchar_p, data, lock=False)\n",
    "    # confirm contents of the shared array\n",
    "    print(array[:10], len(array))\n",
    "    # create a child process\n",
    "    child = Process(target=task, args=(array,ds,))\n",
    "    # start the child process\n",
    "    child.start()\n",
    "    # wait for the child process to complete\n",
    "    child.join()\n",
    "    # check some data in the shared array\n",
    "    print(array[:10], len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TDigest<compression=20.0, size=0.0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array[300]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried with queue, this doesn't really work as it doesn't keep the order of the object. Dont' know if the code below is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000000/5000000 [00:02<00:00, 1927696.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(processes)):\n\u001b[0;32m---> 46\u001b[0m     results\u001b[39m.\u001b[39mappend(queue\u001b[39m.\u001b[39;49mget())\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m process \u001b[39min\u001b[39;00m processes:\n\u001b[1;32m     49\u001b[0m     process\u001b[39m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/.conda/envs/aqua/lib/python3.10/multiprocessing/queues.py:103\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m block \u001b[39mand\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock:\n\u001b[0;32m--> 103\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sem\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/aqua/lib/python3.10/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m maxlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m maxlength \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnegative maxlength\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes(maxlength)\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m buf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m~/.conda/envs/aqua/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_bytes\u001b[39m(\u001b[39mself\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    415\u001b[0m     size, \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m!i\u001b[39m\u001b[39m\"\u001b[39m, buf\u001b[39m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m size \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/aqua/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[39m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m remaining \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[39m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    380\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from multiprocessing import Queue \n",
    "\n",
    "# Define a function to update the digest for a range of indices\n",
    "\n",
    "def update_digest(job_length, ds, digest_list, queue):\n",
    "\n",
    "    [digest_list[j].update(ds[:, j]) for j in range(job_length)]\n",
    "    #print(np.shape(ds))\n",
    "    #print('job_length', job_length)\n",
    "    queue.put(digest_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    digest_list = []\n",
    "    for j in tqdm.tqdm(range(array_length)):\n",
    "       # initalising digests \n",
    "       digest_list.append(TDigest_cr(compression=1))\n",
    "    \n",
    "    # Split the work among processes\n",
    "    num_processes = 22  # this makes it explict \n",
    "    chunk_size = array_length // num_processes\n",
    "    # now adding to the digests   \n",
    "    processes = []\n",
    "    queues = []\n",
    "\n",
    "    queue = Queue() \n",
    "\n",
    "    for i in range(num_processes):\n",
    "        #conn1, conn2 = Pipe()\n",
    "        #queues.append(queue)  # Store the connection objects for later use\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i < num_processes - 1 else array_length\n",
    "        job_length = end - start\n",
    "        job_args = (job_length, ds[:,start:end], digest_list[start:end], queue)\n",
    "        process = Process(target=update_digest, args=job_args)\n",
    "        processes.append(process)\n",
    "\n",
    "    for process in processes:\n",
    "        process.start()\n",
    "\n",
    "    #for queue in queues: \n",
    "        \n",
    "         \n",
    "    results = []\n",
    "    for j in range(len(processes)):\n",
    "        results.append(queue.get())\n",
    "\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "    print('done join')\n",
    "    \n",
    "    #results = []\n",
    "    #for j in range(len(processes)):\n",
    "    #    results.append(queue.get())\n",
    "\n",
    "    print(results)\n",
    "    # new_results = [y for x in results for y in x]\n",
    "\n",
    "    # Now 'results' contains the updated 'digest_list' from all processes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying with Pool as opposed to process, I think the code below is wrong because it takes for ages. The advantage of pool is don't need to worry about the communicator in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000000/5000000 [00:02<00:00, 2066843.80it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate pu for a given (i, j) position\n",
    "from multiprocessing import Pool \n",
    "\n",
    "#global calculate_power_output\n",
    "\n",
    "def update_digests(j, digest_list, ds):\n",
    "    \n",
    "    digest_list[j].update(ds[:, j])\n",
    "\n",
    "    return digest_list[j]\n",
    "\n",
    "digest_list = []\n",
    "\n",
    "for j in tqdm.tqdm(range(array_length)):\n",
    "    # initalising digests \n",
    "    digest_list.append(TDigest_cr(compression=1))\n",
    "        \n",
    "#jobargs = [(j, digest_list, ds) for j in range(array_length)]\n",
    "\n",
    "# Create a pool with the number of desired processes\n",
    "num_processes = 22 # multiprocessing.cpu_count()  # Adjust this value based on your system capabilities\n",
    "#num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Split the work into chunks for parallel processing\n",
    "chunk_size = array_length // num_processes\n",
    "chunks = [(j, digest_list, ds) for j in range(array_length)]\n",
    "\n",
    "with Pool(num_processes) as pool:\n",
    "    start = i * chunk_size\n",
    "    end = (i + 1) * chunk_size if i < num_processes - 1 else array_length\n",
    "    job_length = end - start\n",
    "    jobargs = [(j, digest_list[start:end], ds[start:end,:]) for j in range(job_length)]\n",
    "    #job_args = (job_length, ds[start:end,:], digest_list[start:end])\n",
    "    res = list(pool.starmap(update_digests, jobargs))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel3.9",
   "language": "python",
   "name": "kernel3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
