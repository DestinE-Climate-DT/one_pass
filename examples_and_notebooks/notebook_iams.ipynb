{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32miams\u001b[m\n",
      "  main\u001b[m\n",
      "  output_for_bias_corr\u001b[m\n",
      "  t_digest\u001b[m\n",
      "  test_package\u001b[m\n",
      "l40290.lvt.dkrz.de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_179193/85934964.py:35: UserWarning: Converting non-nanosecond precision datetime values to nanosecond precision. This behavior can eventually be relaxed in xarray, as it is an artifact from pandas which is now beginning to support non-nanosecond precision values. This warning is caused by passing non-nanosecond np.datetime64 or np.timedelta64 values to the DataArray or Variable constructor; it can be silenced by converting the values to nanosecond precision ahead of time.\n",
      "  da['time'] = da.time.astype(\"datetime64[s]\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "import sys \n",
    "import glob \n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "path = \"/home/b/b382291/git/one_pass\"\n",
    "sys.path.append(path)\n",
    "os.chdir(path)\n",
    "\n",
    "from one_pass.convert_time import convert_time\n",
    "from one_pass.check_request import check_request\n",
    "from one_pass import util\n",
    "from one_pass.opa import Opa\n",
    "!git branch \n",
    "\n",
    "### select input and out put directory\n",
    "outdir = '/work/bb1153/b382220/KOSTRA/Tests/iamser_out/'\n",
    "# Check if the directory exists, and create it if not\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "indir = \"/work/bb1153/b382220/KOSTRA/kostrax/DWData/\"\n",
    "file_list = sorted([file for file in os.listdir(indir) if file.endswith('.nc')])\n",
    "\n",
    "!hostname\n",
    "\n",
    "### load data and select variables\n",
    "ds = xr.open_mfdataset(f'{indir}*.nc') ### load data as dask array\n",
    "pr = ds['pr'] ### select data variable for total precipitation\n",
    "da = xr.DataArray(pr)\n",
    "# have to change the precision, data set has strange time stamps\n",
    "da['time'] = da.time.astype(\"datetime64[s]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting a small subset of the data otherwise it's crazy long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date: 2071-01-01T00:30:00.000000000 stop_date: 2071-01-08T01:30:00.000000000\n",
      "start_date: 2071-01-08T02:30:00.000000000 stop_date: 2071-01-15T03:30:00.000000000\n",
      "start_date: 2071-01-15T04:30:00.000000000 stop_date: 2071-01-22T05:30:00.000000000\n",
      "start_date: 2071-01-22T06:30:00.000000000 stop_date: 2071-01-29T07:30:00.000000000\n",
      "start_date: 2071-01-29T08:30:00.000000000 stop_date: 2071-02-05T09:30:00.000000000\n",
      "start_date: 2071-02-05T10:30:00.000000000 stop_date: 2071-02-12T11:30:00.000000000\n",
      "start_date: 2071-02-12T12:30:00.000000000 stop_date: 2071-02-19T13:30:00.000000000\n",
      "start_date: 2071-02-19T14:30:00.000000000 stop_date: 2071-02-26T15:30:00.000000000\n",
      "start_date: 2071-02-26T16:30:00.000000000 stop_date: 2071-03-05T17:30:00.000000000\n",
      "start_date: 2071-03-05T18:30:00.000000000 stop_date: 2071-03-12T19:30:00.000000000\n",
      "start_date: 2071-03-12T20:30:00.000000000 stop_date: 2071-03-19T21:30:00.000000000\n",
      "start_date: 2071-03-19T22:30:00.000000000 stop_date: 2071-03-26T23:30:00.000000000\n",
      "start_date: 2071-03-27T00:30:00.000000000 stop_date: 2071-04-03T01:30:00.000000000\n",
      "start_date: 2071-04-03T02:30:00.000000000 stop_date: 2071-04-10T03:30:00.000000000\n",
      "start_date: 2071-04-10T04:30:00.000000000 stop_date: 2071-04-17T05:30:00.000000000\n",
      "start_date: 2071-04-17T06:30:00.000000000 stop_date: 2071-04-24T07:30:00.000000000\n",
      "start_date: 2071-04-24T08:30:00.000000000 stop_date: 2071-05-01T09:30:00.000000000\n",
      "start_date: 2071-05-01T10:30:00.000000000 stop_date: 2071-05-08T11:30:00.000000000\n",
      "start_date: 2071-05-08T12:30:00.000000000 stop_date: 2071-05-15T13:30:00.000000000\n",
      "start_date: 2071-05-15T14:30:00.000000000 stop_date: 2071-05-22T15:30:00.000000000\n",
      "start_date: 2071-05-22T16:30:00.000000000 stop_date: 2071-05-29T17:30:00.000000000\n",
      "start_date: 2071-05-29T18:30:00.000000000 stop_date: 2071-06-05T19:30:00.000000000\n",
      "start_date: 2071-06-05T20:30:00.000000000 stop_date: 2071-06-12T21:30:00.000000000\n",
      "start_date: 2071-06-12T22:30:00.000000000 stop_date: 2071-06-19T23:30:00.000000000\n",
      "start_date: 2071-06-20T00:30:00.000000000 stop_date: 2071-06-27T01:30:00.000000000\n",
      "start_date: 2071-06-27T02:30:00.000000000 stop_date: 2071-07-04T03:30:00.000000000\n",
      "start_date: 2071-07-04T04:30:00.000000000 stop_date: 2071-07-11T05:30:00.000000000\n",
      "start_date: 2071-07-11T06:30:00.000000000 stop_date: 2071-07-18T07:30:00.000000000\n",
      "start_date: 2071-07-18T08:30:00.000000000 stop_date: 2071-07-25T09:30:00.000000000\n",
      "start_date: 2071-07-25T10:30:00.000000000 stop_date: 2071-08-01T11:30:00.000000000\n",
      "start_date: 2071-08-01T12:30:00.000000000 stop_date: 2071-08-08T13:30:00.000000000\n",
      "start_date: 2071-08-08T14:30:00.000000000 stop_date: 2071-08-15T15:30:00.000000000\n",
      "start_date: 2071-08-15T16:30:00.000000000 stop_date: 2071-08-22T17:30:00.000000000\n",
      "start_date: 2071-08-22T18:30:00.000000000 stop_date: 2071-08-29T19:30:00.000000000\n",
      "start_date: 2071-08-29T20:30:00.000000000 stop_date: 2071-09-05T21:30:00.000000000\n",
      "start_date: 2071-09-05T22:30:00.000000000 stop_date: 2071-09-12T23:30:00.000000000\n",
      "start_date: 2071-09-13T00:30:00.000000000 stop_date: 2071-09-20T01:30:00.000000000\n",
      "start_date: 2071-09-20T02:30:00.000000000 stop_date: 2071-09-27T03:29:59.000000000\n",
      "start_date: 2071-09-27T04:29:59.000000000 stop_date: 2071-10-04T05:29:59.000000000\n",
      "start_date: 2071-10-04T06:29:59.000000000 stop_date: 2071-10-11T07:29:59.000000000\n",
      "start_date: 2071-10-11T08:29:59.000000000 stop_date: 2071-10-18T09:29:59.000000000\n",
      "start_date: 2071-10-18T10:29:59.000000000 stop_date: 2071-10-25T11:29:59.000000000\n",
      "start_date: 2071-10-25T12:29:59.000000000 stop_date: 2071-11-01T13:29:59.000000000\n",
      "start_date: 2071-11-01T14:29:59.000000000 stop_date: 2071-11-08T15:29:59.000000000\n",
      "start_date: 2071-11-08T16:29:59.000000000 stop_date: 2071-11-15T17:29:59.000000000\n",
      "start_date: 2071-11-15T18:29:59.000000000 stop_date: 2071-11-22T19:29:59.000000000\n",
      "start_date: 2071-11-22T20:29:59.000000000 stop_date: 2071-11-29T21:29:59.000000000\n",
      "start_date: 2071-11-29T22:29:59.000000000 stop_date: 2071-12-06T23:29:59.000000000\n",
      "start_date: 2071-12-07T00:29:59.000000000 stop_date: 2071-12-14T01:29:59.000000000\n",
      "start_date: 2071-12-14T02:29:59.000000000 stop_date: 2071-12-21T03:29:59.000000000\n",
      "start_date: 2071-12-21T04:29:59.000000000 stop_date: 2071-12-28T05:29:59.000000000\n",
      "start_date: 2071-12-28T06:29:59.000000000 stop_date: 2072-01-04T07:30:00.000000000\n"
     ]
    }
   ],
   "source": [
    "da = da[:,0:10,0:10]\n",
    "\n",
    "# this is our user requst \n",
    "\n",
    "pass_dic = {\"stat\" : \"iams\",\n",
    "\"percentile_list\" : None,\n",
    "\"thresh_exceed\" : None,\n",
    "\"stat_freq\": \"annually\",\n",
    "\"output_freq\": \"annually\",\n",
    "\"time_step\": 60,\n",
    "\"variable\": \"pr\",\n",
    "\"save\": True,\n",
    "\"checkpoint\": True,\n",
    "\"checkpoint_filepath\": \"/scratch/b/b382291/data/\",\n",
    "\"out_filepath\": \"/scratch/b/b382291/data\"}\n",
    "\n",
    "start = 0\n",
    "stop =  365*24 # end of 2071\n",
    "step = 170 # don't go above this for now \n",
    "\n",
    "opa_stat = Opa(pass_dic)\n",
    "\n",
    "for i in range(start, stop, step): \n",
    "\n",
    "    data = da.isel(time=slice(i,i+step)) # extract moving window 'simulating streaming'\n",
    "    print(f\"start_date: {data.time[0].values} stop_date: {data.time[-1].values}\")\n",
    "\n",
    "    dm = opa_stat.compute(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8760"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opa_stat.n_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison with two pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### all functions\n",
    "############################################################## to select available duration depending on the input data \n",
    "############################################################## time_unit(str) = 'minutes' or 'hours'; time_step(int) = e.g. 1 for hourly, 10 for ten minutes\n",
    "\n",
    "#-- automatically check timestep of files and rescale\n",
    "#-- remove durations that are lower than timestep and round\n",
    "#-- check if duration is a fraction of multiple timesteps\n",
    "def duration_pick(time_unit, time_step):\n",
    "    durations  = [5,10,15,20,30,45,60,90,120,180,240,360,540,720,1080,1440,2880,4320,5760,7200,8640,10080] ### list of all duration windows that may be passed on\n",
    "    \n",
    "    if time_unit != 'minutes': ### format time unit, when not minutes\n",
    "        durations = [d/60 for d in durations]\n",
    "\n",
    "    durations = [d for d in durations if d >= time_step] ### loop over duration windows that fall in the range of the time step\n",
    "    for d in durations:\n",
    "        if d%time_step != 0: ### only select natural numbers\n",
    "            durations.remove(d)  \n",
    "    durations = list(map(int, durations)) ### create list of only intergers\n",
    "    return durations\n",
    "\n",
    "############################################################## compute intensity annual maxima series (iams) for single duration window\n",
    "############################################################## \n",
    "\n",
    "def dm_roller(data, duration, year): ###(ds.'precipitation', list()selected_duration windows, str()year)\n",
    "    \n",
    "    \n",
    "    ### compute rolling sum & select maximum\n",
    "    rolling_sum = data.rolling(time=duration, center=True).sum() ### compute rolling sum for overlapping duration windows\n",
    "    rolling_sum_max = rolling_sum.max(dim='time', skipna=True)  ### use .max() to select sum with maximum value\n",
    "    \n",
    "    ### add coordinates to the array\n",
    "    if time_unit != 'minutes':\n",
    "        rolling_sum_max = rolling_sum_max.expand_dims(duration = ([duration*60])) ### if not minutes (hourly) transform windows back to minutes \n",
    "    else: \n",
    "        rolling_sum_max = rolling_sum_max.expand_dims(duration = ([duration])) ### add coordinate duration to data array\n",
    "        \n",
    "    #year= np.unique(data.time.dt.year)    \n",
    "    rolling_sum_max = rolling_sum_max.expand_dims(year = ([year])) ### coordinate year\n",
    "    \n",
    "    return rolling_sum_max\n",
    "\n",
    "############################################################## For data of several years: compute intensity annual maxima series (iams) for single duration window\n",
    "##############################################################\n",
    "\n",
    "def iamser(data, years, durations):###(ds.'precipitation', list()selected_duration windows, list()years)\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}): ### set dask to slice large chunks 'False' also works dunno\n",
    "        # Your array indexing/slicing operations\n",
    "        all_years_iams = [] ### create list to gather yearly arrays\n",
    "        data_grouped = data.groupby('time.year') ### group data into years\n",
    "        #years= list(np.unique(pr.time.dt.year)) ### create list with all years\n",
    "        for y in years: ### loop over years in year list\n",
    "            print(y)\n",
    "            #group_with_time = group.assign_coords(time=group['time'])\n",
    "            dmax_list = [] ### list to add each duartion sum max\n",
    "            for durs in durations: ### loop over selected durations\n",
    "                dmax = dm_roller(data_grouped[y], durs, y) ### call iamser function for one year\n",
    "                dmax_list.append(dmax) ### append iams output to iams_list\n",
    "            iamsfile = xr.concat(dmax_list, dim='duration') ### concat all durations into one array\n",
    "            all_years_iams.append (iamsfile) ### add entire year to list \n",
    "\n",
    "        #iams = xr.concat(all_years_iams, dim='year') ### final concat, all years with all durations as one array\n",
    "        #return iams \n",
    "        return all_years_iams ### reuturn list, easier to save data as .nc year for year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2071\n"
     ]
    }
   ],
   "source": [
    "### pick possible duration windows\n",
    "time_step = 1 ### define data temporal fruequency\n",
    "time_unit = 'hour' ### define data time unit\n",
    "sel_durs = duration_pick('hours',1) ### call sel_durs to select all possible windows within given time frquency & unit\n",
    "\n",
    "### load data and select variables\n",
    "ds = xr.open_mfdataset(f'{indir}*.nc') ### load data as dask array\n",
    "#ds = xr.open_mfdataset(f'{indir}{file_list[4]}')\n",
    "pr = ds['pr'] ### select data variable for total precipitationpr\n",
    "pr= pr[:,0:10,0:10]\n",
    "#pr = ds['tp']\n",
    "years= [2071] # list(np.unique(pr.time.dt.year)) ### create list for all years in the given data set \n",
    "\n",
    "### run functions to create intensity annual  maxima series\n",
    "iams = iamser(pr, years, sel_durs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<xarray.DataArray 'pr' (year: 1, duration: 15, rlat: 10, rlon: 10)>\n",
       " dask.array<concatenate, shape=(1, 15, 10, 10), dtype=float32, chunksize=(1, 1, 10, 10), chunktype=numpy.ndarray>\n",
       " Coordinates:\n",
       "   * year      (year) int64 2071\n",
       "   * duration  (duration) int64 60 120 180 240 360 ... 4320 5760 7200 8640 10080\n",
       "   * rlon      (rlon) float64 -10.29 -10.27 -10.24 -10.21 ... -10.1 -10.07 -10.05\n",
       "   * rlat      (rlat) float64 -4.496 -4.469 -4.441 ... -4.304 -4.276 -4.249\n",
       "     lat       (rlat, rlon) float64 dask.array<chunksize=(10, 10), meta=np.ndarray>\n",
       "     lon       (rlat, rlon) float64 dask.array<chunksize=(10, 10), meta=np.ndarray>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iams[0].values[0,6,5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04076696652381135"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.pr.values[0,6,5,2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel3.9",
   "language": "python",
   "name": "kernel3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
