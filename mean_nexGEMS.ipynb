{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses already re-gridded nexGEMS (given as netCDF) data as input to test the one-pass mean algorithm. \n",
    "\n",
    "The structure of the code is as follows: \n",
    "\n",
    "1. User sets: \n",
    "    - variable to calculate the algorithm on e.g. 'tas'\n",
    "    - frequency of mean e.g. 'daily mean' \n",
    "    - saving requirements e.g. save to disk or output into memory \n",
    "\n",
    "\n",
    "2. These variables are fed into the function where we simulate the first 'MARS request', how data will be extracted from the GSV. Having the MARS request inside the function means that it will not be possible to extract part of the statistic while it is being calculated (e.g. getting the the daily mean after only 12 hours of data), all the data must be read in and calculated before the code will exit the function. \n",
    "\n",
    "3. The code checks if the input data (here moving time window, in future GRIB data from MARS) is the 'first' e.g. if you are calculating the daily mean, check if it is the first piece of data for that day. If this is true, a loop will initalise three key variables; \n",
    "    - count: the number of peices of information that have been read into the statistic (starts at 0)\n",
    "    - nData: the number of peices of information required to be read until the statistic is complete (e.g. daily mean with 1 hour spaced data requires 24 pieces of information)\n",
    "    - mean: initalised empty array of the same size as the lat, lon data. \n",
    "\n",
    "4. The data is then fed into the actual algorithm that calculates the 'rolling' mean. This is embedded in a loop that will run until count == nData, so all the required data has been read. \n",
    "\n",
    "5. Once this loop is finished, the new mean statistic will be added to a new dataSet in xarray, of a similar structure to the original dataSet. Depending on the save options, this will either be returned from the function in memory or saved as a netCDF file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries \n",
    "import numpy as np\n",
    "import xarray as xr \n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#import sys\n",
    "#import time\n",
    "#import dask\n",
    "#import os\n",
    "#import cfgrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual one-pass function for calculating the mean statistic \n",
    "# input is the new data set (as numpy array)\n",
    "# mean is the previously calculated mean (same shape as input, all zeros if first one, again numpy array)\n",
    "# count is the actual number of values that have been input into the one-pass \n",
    "\n",
    "def meanFun(input, mean, count):\n",
    "\n",
    "    # now to update the acutal mean:\n",
    "    # can do this element use (python) or using numpy, should check speed but I think python is faster\n",
    "    mean += (input - mean)/(count+1) # python method \n",
    "    #mean += np.subtract(input, mean)/(count+1) #numpy method\n",
    "    count += 1 # updating the count \n",
    "    \n",
    "    # checker to see the loop is working\n",
    "    #print(count)\n",
    "    \n",
    "    return mean, count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions for Iker: \n",
    "1. Will each 'data block' contain multiple variables or just one?\n",
    "2. How many GRIB messages (i.e. time steps) per block? If there is multiple in the file, might need to run np.mean over the data first then include that in the one-pass mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function includes all the meta data checks and calculations for count and nData \n",
    "\n",
    "def meanOPA(dsFull, meanFreq, var, save):\n",
    "\n",
    "    # defining these as arbitary values so that we can start the while loop, these will be redefinded on the first loop\n",
    "    nData = 1000\n",
    "    count = 0\n",
    "\n",
    "    while(count < nData):\n",
    "\n",
    "        # HERE YOU HAVE THE MARS REQUESTS FOR DATA \n",
    "        # simulating streamed data by just taking a time slice \n",
    "        ds = dsFull.isel(time=slice(count,(count+1))) # slice(start, stop, step), extract 'moving window' which is hourly data\n",
    "\n",
    "        # currently ds is the whole Dataset with all variables so want to filter for use one variable, \n",
    "        # below code only works on a dataArray not on a full dataSet \n",
    "        # converting dataSet into dataArray LIST OF ALL POTENTIAL GRIB VARIABLES THAT IT COULD BE \n",
    "        if(var == \"tas\"):\n",
    "            ds = ds.tas\n",
    "        elif(var == \"uas\"):\n",
    "            ds = ds.uas\n",
    "        elif(var == \"vas\"):\n",
    "            ds = ds.vas\n",
    " \n",
    "        # this will work even with 1 input for time, but data must still have a time dimension \n",
    "        timeStampList = sorted(ds.time.data) # this method could be very different for a GRIB file, need to understand time stamps \n",
    "        # this exracts all the timestamps of each data point in hours converting to a pandas timestamp from np.datetime64\n",
    "        timeStampPandas = [pd.to_datetime(x) for x in timeStampList]\n",
    "        timeStamp = timeStampPandas[0]\n",
    "\n",
    "        # # checking that the spacing of the data is indeed hours\n",
    "        # will need to do this differently when you only have 1 GRIB message - e.g. one time stamp\n",
    "        # this is currently using the numpy version of the list, maybe re-write to work on Pandas date time list\n",
    "        # TIMESTEP SHOUND BE KNOWN APRIORI FROM THE MARS REQUEST\n",
    "        timeStep = np.float64(60) # converting timeStep into minutes, float 64\n",
    "        #timeStampList[1] - timeStampList[0] # this is assuming there are a few GRIB mesages per data block \n",
    "        \n",
    "        # need to decide what your default time unit is going to be \n",
    "        # THIS IS CRITICAL FOR NOW CHOOSING MINUTES\n",
    "        timeStep = (timeStep.astype('timedelta64[m]') / np.timedelta64(1, 'm')) # converting timeStep into minutes, float 64\n",
    "\n",
    "        # loop to calculate nData, based on timeStamp and timeStep \n",
    "        # want to check what the mean frequency is, converting data to integer value for looping \n",
    "        # - this could be problematic if not wholy divisable\n",
    "        if(meanFreq == \"hourly\"): # i.e hourly means\n",
    "            # first thing to check is if this is the first in the series, time stamp must be less than 60 \n",
    "            timeStampMin = timeStamp.minute\n",
    "            \n",
    "            if(timeStampMin == timeStep): # this indicates that it's the first data of the day otherwise timeStamp will be larger\n",
    "                # calculated by hour freq of mean / timestep (hours) of data\n",
    "                nData = int(60/timeStep) # number of elements of data that need to be added to the cumulative mean \n",
    "                count = 0 # first in the loop \n",
    "                mean = np.zeros((np.size(ds.lat), np.size(ds.lon))) # only initalise cumulative mean if you know this is the first input\n",
    "                #timeDaily = pd.DatetimeIndex([]) # initalising time loop \n",
    "\n",
    "            elif(timeStep > 60):\n",
    "                # we have a problem \n",
    "                print('timeStep too large for hourly means')\n",
    "\n",
    "        elif(meanFreq == \"daily\"):\n",
    "            # first check if the timeStep is less than an hour, in which case need to count the minutes \n",
    "            if(timeStep < 60):\n",
    "                timeStampMin = timeStamp.minute\n",
    "                if(timeStampMin == timeStep): # this indicates that it's the first, works when comparing float64 to int\n",
    "                    nData = int(24*60/timeStep) # number of elements of data that need to be added to the cumulative mean \n",
    "                    count = 0\n",
    "                    mean = np.zeros((np.size(ds.lat), np.size(ds.lon))) # only initalise cumulative mean if you know this is the first input\n",
    "                    #timeDaily = pd.DatetimeIndex([]) # initalising time loop \n",
    "\n",
    "            elif(timeStep < 60*24): # time step is less than a day \n",
    "                timeStampHour = timeStamp.hour*60 # converting to minutes \n",
    "                \n",
    "                # THIS LINE NEEDS TO CHECK IF IT'S THE FIRST DATA ELEMENT COMING IN\n",
    "                # NEED TO FIND A MORE ROBUST WAY OF DOING THIS\n",
    "                if(timeStampHour == 0): # this indicates that it's the first, works when comparing float64 to int,\n",
    "                    nData = int(24*60/timeStep) # number of elements of data that need to be added to the cumulative mean \n",
    "                    count = 0\n",
    "                    mean = np.zeros((np.size(ds.lat), np.size(ds.lon))) # only initalise cumulative mean if you know this is the first input\n",
    "                    #timeDaily = pd.DatetimeIndex([]) # initalising time loop \n",
    "\n",
    "        elif(meanFreq == \"weekly\"):\n",
    "            nData = int(24*7*60/timeStep) # is there ever not 7 days in a week? \n",
    "            # NOT FINISHED \n",
    "\n",
    "        elif(meanFreq == \"monthly\"): \n",
    "            # NOT FINISHED \n",
    "            # need to check the month of input before making calculation \n",
    "            # this extracts the month of the date, need to check this works with different versions of numpy \n",
    "            month = timeStampPandas[0].month\n",
    "            if (month == 1 or month == 3 or month == 5 or month == 7 or month == 8 or month == 10 or month == 12):\n",
    "                # jan, mar, may, july, aug, oct, dec all have 31 days \n",
    "                nData = int((31*24*60)/timeStep)\n",
    "            elif(month == 4 or month == 6 or month == 9 or month == 11):\n",
    "                nData = int((30*24*60)/timeStep)\n",
    "                # april, june, sep, nov, all have 30 days \n",
    "            elif(month == 2):\n",
    "                # then need to check year for leap year ADD \n",
    "                nData = int((28*24*60)/timeStep)\n",
    "\n",
    "        # if none of the if statements above are entered then none of the arrays will be re-initalised and the code will carry on \n",
    "        # assuming that the cumulative mean is saved in memory \n",
    "\n",
    "\n",
    "        # incoming into this you ALWAYS have:\n",
    "        #  ds - new dataArray (no longer dataSet as you have extracted the variable of interest)\n",
    "        #  mean - cumulative, might be an empty array if this is the first time this has run\n",
    "        #  nData - number of data points required until the mean is full \n",
    "        #  count - number of data points that have been read \n",
    "\n",
    "        ## HERE CONVERTING THE NEW DATA INTO A NUMPY ARRAY \n",
    "        # this may need to be more robust if multiple heights or other dimensions \n",
    "        ds = np.squeeze(ds).data # if there are multiple heights in the same file, this will remove redundant 1 dimensions and .data extracts numpy array\n",
    "\n",
    "\n",
    "        ## RUNNING THROUGH ACTUAL MEAN ALGORITHM \n",
    "        mean, count = meanFun(ds, mean, count)\n",
    "            \n",
    "            # what would need to be saved for a restart file? \n",
    "            # RESTART FILES?\n",
    "\n",
    "    ## NOW SAVING - DON'T NEED A CONDITION AS WHILE LOOP HAS FINISHED \n",
    "    \n",
    "    #meanDaily = np.insert(meanDaily, countDays,  mean, axis = 0) # this is if you want to put multiple days in one file \n",
    "    #meanDaily = np.vstack((meanDaily, mean))\n",
    "    #timeDaily[0] = timeStamp.date()\n",
    "    #timeDaily = pd.DatetimeIndex.insert(timeDaily, countDays, timeStampDaily[i])\n",
    "\n",
    "    #countDays = countDays + 1\n",
    "    mean = np.expand_dims(mean, axis=0) # adding back extra time dimension \n",
    "\n",
    "    dsFull.attrs[\"OPA\"] = \"daily mean calculated using one-pass algorithm\"\n",
    "    attrs = dsFull.attrs\n",
    "\n",
    "    # converting the mean into a new dataArray \n",
    "    dm = xr.Dataset(\n",
    "    data_vars = dict(\n",
    "        tas_Mean = ([\"time\",\"lat\",\"lon\"], mean),    # need to add variable attributes                         \n",
    "    ),\n",
    "    coords = dict(\n",
    "        time = ([\"time\"], [pd.to_datetime(timeStamp.date())]),\n",
    "        lon = ([\"lon\"], dsFull.lon.data),\n",
    "        lat = ([\"lat\"], dsFull.lat.data),\n",
    "    ),\n",
    "    attrs = attrs\n",
    "    )\n",
    "\n",
    "    if(save == \"true\"):\n",
    "        # save new DataSet as netCDF \n",
    "        newFile = \"/esarchive/scratch/kgrayson/git/onepass_development/mean/nexGEMS/tas_daily_means.nc\"\n",
    "        #f = open(newFile, \"w\")\n",
    "        dm.to_netcdf(path = newFile)\n",
    "        dm.close() \n",
    "        print('finished saving')\n",
    "    else: \n",
    "        return dm\n",
    "        # output DataSet in memory, should look like incoming file but with some attributes changed \n",
    "\n",
    "# time data = dask.delayed(mean(ds, meanFreq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with nextGEMS original files \n",
    "#filePath = \"/esarchive/exp/mpi-esm1-2-xxr/nextgems/original_files/\"\n",
    "\n",
    "filePath = \"/esarchive/scratch/alacima/python/destination_earth/icon/*.nc\" # this has already been re-gridded onto regular lat / lon grid\n",
    "#fileList = sorted(os.listdir(filePath)) # sorted to get them into the correct order \n",
    "fileList = glob.glob(filePath) # glob function used to just extract the netCDF files \n",
    "fileList.sort() # sorted to get them into the correct order \n",
    "\n",
    "nFiles = np.size(fileList) # finding number of files (also number of months)\n",
    "#fileName = filePath + fileList[0]\n",
    "\n",
    "ds = xr.open_dataset(fileList[0]) # open dataset \n",
    "\n",
    "#, chunks={\"values\": \"auto\"}\n",
    "#ds = xr.open_dataset(filePath,engine='cfgrib',backend_kwargs={'filter_by_keys':{'typeOfLevel': 'surface'}})\n",
    "\n",
    "#ds = xr.open_dataset(fileName, engine = \"netcdf4\", chunks={\"lat\": 100, \"lon\": 100}) # open dataset \n",
    "# adding in chunks so that the data set is filled with dask arrays, might want to use auto chunking \n",
    "#, chunks={\"lat\": \"auto\", \"lon\": \"auto\"}\n",
    "\n",
    "# what variables need to be passed to the function? \n",
    "meanFreq = \"daily\"\n",
    "var = \"tas\"\n",
    "save = \"false\"\n",
    "\n",
    "# actual function for OPA \n",
    "dm = meanOPA(ds, meanFreq, var, save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.DataArray &#x27;tas_Mean&#x27; ()&gt;\n",
       "array(0.00010681)\n",
       "Coordinates:\n",
       "    time     datetime64[ns] 2020-05-01</pre>"
      ],
      "text/plain": [
       "<xarray.DataArray 'tas_Mean' ()>\n",
       "array(0.00010681)\n",
       "Coordinates:\n",
       "    time     datetime64[ns] 2020-05-01"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting OPA mean from newly calculated dataSet\n",
    "mean = np.squeeze(dm.tas_Mean)\n",
    "\n",
    "# calcuating normal mean from the full data set \n",
    "npMean = np.mean(ds.tas, axis = 0)\n",
    "npMean = np.squeeze(npMean)\n",
    "\n",
    "# checking max error across the whole data set\n",
    "np.max(mean - npMean.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fixing problem with xarray0.17\n",
    "count = 0\n",
    "ds = ds.isel(time=slice(count,(count+1))) # slice(start, stop, step), extract 'moving window' which is hourly data\n",
    "\n",
    "# this will work even with 1 input for time, but data must still have a time dimension \n",
    "timeStampList = sorted(ds.time.data) # this method could be very different for a GRIB file, need to understand time stamps \n",
    "# this exracts all the timestamps of each data point in hours converting to a pandas timestamp from np.datetime64\n",
    "timeStampPandas = [pd.to_datetime(x) for x in timeStampList]\n",
    "timeStamp = timeStampPandas[0]\n",
    "\n",
    "mean = np.zeros((np.size(ds.lat), np.size(ds.lon))) # only initalise cumulative mean if you know this is the first input\n",
    "\n",
    "mean = np.expand_dims(mean, axis=0) # adding back extra time dimension \n",
    "\n",
    "\n",
    "ds.attrs[\"OPA\"] = \"daily mean calculated using one-pass algorithm\"\n",
    "attrs = ds.attrs\n",
    "\n",
    "# converting the mean into a new dataArray \n",
    "dm = xr.Dataset(\n",
    "data_vars = dict(\n",
    "    tas_Mean = ([\"time\",\"lat\",\"lon\"], mean),    # need to add variable attributes                         \n",
    "),\n",
    "coords = dict(\n",
    "    time = ([\"time\"], [pd.to_datetime(timeStamp.date())]),\n",
    "    lon = ([\"lon\"], ds.lon.data),\n",
    "    lat = ([\"lat\"], ds.lat.data),\n",
    "),\n",
    "attrs = attrs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'standard_name': 'tas',\n",
       " 'long_name': 'temperature in 2m',\n",
       " 'units': 'K',\n",
       " 'param': '0.0.0',\n",
       " 'cell_methods': 'time: mean'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ds.assign_coords(time = (\"time\", pd.to_datetime(timeStamp.date())))\n",
    "\n",
    "ds.tas.attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
